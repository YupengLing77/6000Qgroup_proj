"""
æ‰¹é‡å¯è§†åŒ–æµ‹è¯•é›†ï¼Œç”Ÿæˆ HTML æŠ¥å‘Š
"""
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from peft import PeftModel
from PIL import Image, ImageDraw, ImageFont
import json
import re
import os
from tqdm import tqdm
import base64
from io import BytesIO

def parse_bbox(response_text):
    """ä»æ¨¡å‹å›ç­”ä¸­æå– bbox åæ ‡"""
    json_match = re.search(r'\{\s*"bbox_2d"\s*:\s*\[(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\]', response_text)
    if json_match:
        return [int(x) for x in json_match.groups()]
    array_match = re.search(r'\[(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\]', response_text)
    if array_match:
        return [int(x) for x in array_match.groups()]
    return None

def calculate_iou(box1, box2):
    """è®¡ç®— IoU"""
    if not box1 or not box2:
        return 0.0
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

def image_to_base64(image):
    """å°† PIL å›¾åƒè½¬ä¸º base64"""
    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode()

def draw_bbox(image, bbox, color, label):
    """åœ¨å›¾åƒä¸Šç»˜åˆ¶ bbox"""
    if not bbox:
        return image
    draw = ImageDraw.Draw(image)
    x1, y1, x2, y2 = bbox
    draw.rectangle([x1, y1, x2, y2], outline=color, width=4)
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 18)
    except:
        font = ImageFont.load_default()
    text_bbox = draw.textbbox((x1, y1-22), label, font=font)
    draw.rectangle([text_bbox[0]-2, text_bbox[1]-2, text_bbox[2]+2, text_bbox[3]+2], fill=color)
    draw.text((x1, y1-22), label, fill="white", font=font)
    return image

def batch_visualize(test_json, base_model_path, lora_checkpoint, num_samples=5, output_dir="batch_visualization"):
    """æ‰¹é‡å¯è§†åŒ–æµ‹è¯•"""
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open(test_json, 'r') as f:
        test_data = json.load(f)
    
    # åªé€‰æ‹©æ£€æµ‹ä»»åŠ¡çš„æ ·æœ¬ï¼ˆåŒ…å« "Locate" æˆ– "bbox" çš„ï¼‰
    detection_samples = []
    for sample in test_data:
        question = sample['conversations'][0]['value']
        if 'Locate' in question or 'bbox' in question.lower():
            detection_samples.append(sample)
    
    print(f"æ€»æ ·æœ¬æ•°: {len(test_data)}")
    print(f"æ£€æµ‹ä»»åŠ¡æ ·æœ¬: {len(detection_samples)}")
    print(f"å°†æµ‹è¯•å‰ {num_samples} ä¸ªæ£€æµ‹æ ·æœ¬\n")
    
    # åªå–å‰ num_samples ä¸ªä¸åŒçš„å›¾åƒ
    selected_samples = detection_samples[:num_samples]
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModelForVision2Seq.from_pretrained(
        base_model_path, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True
    )
    base_processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)
    base_model.eval()
    
    # åŠ è½½ LoRA æ¨¡å‹ï¼ˆç‹¬ç«‹å®ä¾‹ï¼‰
    print("åŠ è½½ LoRA æ¨¡å‹...")
    lora_base = AutoModelForVision2Seq.from_pretrained(
        base_model_path, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True
    )
    lora_model = PeftModel.from_pretrained(lora_base, lora_checkpoint)
    lora_processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)
    lora_model.eval()
    
    results = []
    
    for i, sample in enumerate(tqdm(selected_samples, desc="å¤„ç†ä¸­")):
        image_path = sample['image']
        gt_text = sample['conversations'][1]['value']
        gt_bbox = parse_bbox(gt_text)
        
        print(f"\næ ·æœ¬ {i}: {image_path}")
        print(f"  Ground Truth: {gt_bbox}")
        
        image = Image.open(image_path).convert("RGB")
        
        # æ„å»ºæç¤º
        prompt = "Locate the logo in this image and output the bbox coordinates in JSON format."
        
        # åŸºç¡€æ¨¡å‹é¢„æµ‹
        messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": prompt}]}]
        text = base_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = base_processor(text=[text], images=[image], return_tensors="pt").to("cuda")
        
        print("  é¢„æµ‹åŸºç¡€æ¨¡å‹...")
        with torch.no_grad():
            base_output = base_model.generate(**inputs, max_new_tokens=256, do_sample=False)
        base_response = base_processor.batch_decode(base_output, skip_special_tokens=True)[0].split("assistant\n")[-1].strip()
        base_bbox = parse_bbox(base_response)
        print(f"  åŸºç¡€æ¨¡å‹: {base_bbox}")
        
        # LoRA æ¨¡å‹é¢„æµ‹ï¼ˆé‡æ–°æ„å»º inputsï¼‰
        messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": prompt}]}]
        text = lora_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = lora_processor(text=[text], images=[image], return_tensors="pt").to("cuda")
        
        print("  é¢„æµ‹ LoRA æ¨¡å‹...")
        with torch.no_grad():
            lora_output = lora_model.generate(**inputs, max_new_tokens=256, do_sample=False)
        lora_response = lora_processor.batch_decode(lora_output, skip_special_tokens=True)[0].split("assistant\n")[-1].strip()
        lora_bbox = parse_bbox(lora_response)
        print(f"  LoRA æ¨¡å‹: {lora_bbox}")
        print(f"  Ground Truth: {gt_bbox}")
        
        # è®¡ç®— IoU
        base_iou = calculate_iou(gt_bbox, base_bbox) if gt_bbox else 0
        lora_iou = calculate_iou(gt_bbox, lora_bbox) if gt_bbox else 0
        
        # ç»˜åˆ¶å¯¹æ¯”å›¾
        vis_image = image.copy()
        if gt_bbox:
            vis_image = draw_bbox(vis_image, gt_bbox, "red", "GT")
        if base_bbox:
            vis_image = draw_bbox(vis_image, base_bbox, "blue", f"Base IoU={base_iou:.2f}")
        if lora_bbox:
            vis_image = draw_bbox(vis_image, lora_bbox, "green", f"LoRA IoU={lora_iou:.2f}")
        
        vis_path = os.path.join(output_dir, f"sample_{i:03d}.jpg")
        vis_image.save(vis_path)
        print(f"  ä¿å­˜: {vis_path}")
        
        results.append({
            'id': i,
            'image': image_path,
            'gt_bbox': gt_bbox,
            'base_bbox': base_bbox,
            'lora_bbox': lora_bbox,
            'base_iou': base_iou,
            'lora_iou': lora_iou,
            'vis_path': f"sample_{i:03d}.jpg",
            'image_b64': image_to_base64(vis_image),
            'base_response': base_response,
            'lora_response': lora_response
        })
    
    # ç”Ÿæˆ HTML æŠ¥å‘Š
    html = f"""
    <!DOCTYPE html>
    <html><head><meta charset="utf-8"><title>æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š</title>
    <style>
        body {{font-family: Arial; margin: 20px;}}
        .summary {{background: #f0f0f0; padding: 15px; margin-bottom: 20px; border-radius: 5px;}}
        .sample {{border: 1px solid #ddd; margin: 20px 0; padding: 15px; border-radius: 5px;}}
        .sample img {{max-width: 800px; border: 1px solid #ccc;}}
        .metrics {{display: flex; gap: 20px; margin: 10px 0;}}
        .metric {{background: #e8f4f8; padding: 10px; border-radius: 3px;}}
        .better {{color: green; font-weight: bold;}}
        .worse {{color: red;}}
    </style></head><body>
    <h1>ğŸ¯ LoRA å¾®è°ƒæ•ˆæœå¯¹æ¯”æŠ¥å‘Š</h1>
    <div class="summary">
        <h2>æ€»ä½“ç»Ÿè®¡</h2>
        <p>æµ‹è¯•æ ·æœ¬æ•°: {len(results)}</p>
        <p>å¹³å‡ IoU (åŸºç¡€æ¨¡å‹): {sum(r['base_iou'] for r in results)/len(results):.3f}</p>
        <p>å¹³å‡ IoU (LoRAæ¨¡å‹): {sum(r['lora_iou'] for r in results)/len(results):.3f}</p>
        <p class="{"better" if sum(r['lora_iou'] for r in results) > sum(r['base_iou'] for r in results) else "worse"}">
            æå‡: {(sum(r['lora_iou'] for r in results) - sum(r['base_iou'] for r in results))/len(results):.3f}
        </p>
    </div>
    """
    
    for r in results:
        better = "better" if r['lora_iou'] > r['base_iou'] else "worse"
        html += f"""
        <div class="sample">
            <h3>æ ·æœ¬ {r['id'] + 1} - {os.path.basename(r['image'])}</h3>
            <img src="data:image/jpeg;base64,{r['image_b64']}">
            <div class="metrics">
                <div class="metric">Ground Truth: {r['gt_bbox']}</div>
                <div class="metric">åŸºç¡€æ¨¡å‹ IoU: {r['base_iou']:.3f}</div>
                <div class="metric {better}">LoRAæ¨¡å‹ IoU: {r['lora_iou']:.3f}</div>
            </div>
            <details>
                <summary>æŸ¥çœ‹è¯¦ç»†å“åº”</summary>
                <p><strong>ğŸ”µ åŸºç¡€æ¨¡å‹:</strong><br>{r['base_response'][:200]}...</p>
                <p><strong>ğŸŸ¢ LoRAæ¨¡å‹:</strong><br>{r['lora_response'][:200]}...</p>
            </details>
            <p>ğŸ”µ åŸºç¡€é¢„æµ‹: {r['base_bbox']}</p>
            <p>ğŸŸ¢ LoRAé¢„æµ‹: {r['lora_bbox']}</p>
            <p>ğŸ”´ çœŸå®æ ‡æ³¨: {r['gt_bbox']}</p>
        </div>
        """
    
    html += "</body></html>"
    
    report_path = os.path.join(output_dir, "report.html")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(html)
    
    print(f"\nâœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"å¹³å‡ IoU æå‡: {(sum(r['lora_iou'] for r in results) - sum(r['base_iou'] for r in results))/len(results):.3f}")

if __name__ == "__main__":
    batch_visualize(
        "logo_test.json",
        "Qwen/Qwen3-VL-2B-Instruct",
        "/home/jiahuawang/test/classVLM/output/qwen3-vl-2b-logo-lora/checkpoint-564",
        num_samples=10
    )
