# Qwen3-VL Logo Detection Fine-tuning

åŸºäº Qwen3-VL-2B-Instruct çš„ Logo æ£€æµ‹å’Œè¯†åˆ«ä»»åŠ¡ LoRA å¾®è°ƒé¡¹ç›®ã€‚

## ğŸ“‹ ç›®å½•

- [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†æ­¥éª¤](#è¯¦ç»†æ­¥éª¤)
- [è¯„ä¼°ä¸åˆ†æ](#è¯„ä¼°ä¸åˆ†æ)

---

## ğŸ”§ ç¯å¢ƒè¦æ±‚

### å¿…éœ€ä¾èµ–

```bash
pip install torch==2.6.0 torchvision==0.21.0
pip install transformers>=4.57.0
pip install deepspeed==0.17.1
pip install accelerate==1.7.0
pip install peft==0.17.1
pip install flash-attn==2.7.4.post1
pip install triton==3.2.0
pip install torchcodec==0.2
pip install datasets pillow tqdm
```

### ç¡¬ä»¶è¦æ±‚

- **GPU**: è‡³å°‘ 1 å¼  24GB æ˜¾å­˜ GPUï¼ˆå¦‚ RTX 3090/4090, A100ï¼‰
- **å­˜å‚¨**: è‡³å°‘ 50GB å¯ç”¨ç©ºé—´

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### âš ï¸ é¦–æ¬¡ä½¿ç”¨å¿…é¡»é…ç½®è·¯å¾„

åœ¨å¼€å§‹è®­ç»ƒå‰ï¼Œéœ€è¦ä¿®æ”¹ä»¥ä¸‹æ–‡ä»¶ä¸­çš„ç»å¯¹è·¯å¾„ï¼š

#### 1. è®­ç»ƒè„šæœ¬è·¯å¾„

**æ–‡ä»¶**: `train_part/train_logo_lora.sh`

```bash
# ç¬¬ 7 è¡Œï¼šä¿®æ”¹ä¸ºä½ çš„é¡¹ç›®æ ¹ç›®å½•
cd /home/YOUR_USERNAME/YOUR_PATH/classVLM
```

#### 2. æ•°æ®é›†é…ç½®è·¯å¾„

**æ–‡ä»¶**: `Qwen3-VL/qwen-vl-finetune/qwenvl/data/__init__.py`

```python
# ç¬¬ 31 è¡Œå’Œç¬¬ 37 è¡Œï¼šä¿®æ”¹ä¸ºä½ çš„é¡¹ç›®è·¯å¾„
LOGO_DATASET = {
    "annotation_path": "/home/YOUR_USERNAME/YOUR_PATH/classVLM/train_subset.json",
    "data_path": "",
}

LOGO_FULL = {
    "annotation_path": "/home/YOUR_USERNAME/YOUR_PATH/classVLM/logo_train.json",
    "data_path": "",
}
```

**å¿«é€Ÿæ›¿æ¢å‘½ä»¤**:
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œï¼Œè‡ªåŠ¨æ›¿æ¢æ‰€æœ‰è·¯å¾„
PROJECT_PATH=$(pwd)  # è·å–å½“å‰ç›®å½•çš„ç»å¯¹è·¯å¾„
sed -i "s|/home/jiahuawang/test/classVLM|${PROJECT_PATH}|g" train_part/train_logo_lora.sh
sed -i "s|/home/jiahuawang/test/classVLM|${PROJECT_PATH}|g" Qwen3-VL/qwen-vl-finetune/qwenvl/data/__init__.py
```

**å‘½ä»¤è§£é‡Š**:
- `PROJECT_PATH=$(pwd)`: è·å–å½“å‰ç›®å½•çš„ç»å¯¹è·¯å¾„å¹¶ä¿å­˜åˆ°å˜é‡
- `sed -i`: ç›´æ¥ä¿®æ”¹æ–‡ä»¶ï¼ˆä¸åŠ  `-i` åªä¼šè¾“å‡ºåˆ°å±å¹•ï¼‰
- `"s|æ—§æ–‡æœ¬|æ–°æ–‡æœ¬|g"`: æ›¿æ¢å‘½ä»¤
  - `s` = substituteï¼ˆæ›¿æ¢ï¼‰
  - `|` = åˆ†éš”ç¬¦ï¼ˆä¹Ÿå¯ç”¨ `/`ï¼Œä½†è·¯å¾„ä¸­æœ‰ `/` æ‰€ä»¥ç”¨ `|` æ›´æ¸…æ™°ï¼‰
  - `/home/jiahuawang/test/classVLM` = è¦æŸ¥æ‰¾çš„æ—§è·¯å¾„
  - `${PROJECT_PATH}` = æ›¿æ¢æˆçš„æ–°è·¯å¾„ï¼ˆä½ çš„å®é™…é¡¹ç›®è·¯å¾„ï¼‰
  - `g` = globalï¼ˆæ›¿æ¢æ–‡ä»¶ä¸­æ‰€æœ‰åŒ¹é…é¡¹ï¼Œä¸åªæ˜¯ç¬¬ä¸€ä¸ªï¼‰

**æ‰‹åŠ¨ä¿®æ”¹æ–¹å¼**ï¼ˆå¦‚æœä¸æƒ³ç”¨å‘½ä»¤ï¼‰:
```bash
# ç”¨æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€æ–‡ä»¶ï¼ŒæŠŠæ‰€æœ‰ /home/jiahuawang/test/classVLM 
# æ”¹æˆä½ çš„å®é™…è·¯å¾„ï¼Œæ¯”å¦‚ /home/yourname/projects/classVLM
vim train_part/train_logo_lora.sh
vim Qwen3-VL/qwen-vl-finetune/qwenvl/data/__init__.py
```

---

### è®­ç»ƒæµç¨‹

```bash
# 1. ä¸‹è½½æ•°æ®é›†
python download.py

# 2. è½¬æ¢æ•°æ®æ ¼å¼ï¼ˆä¸€æ¬¡æ€§æ“ä½œï¼‰
python logo_data_oral/convert_logo_data.py

# 3. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆ10Kæ ·æœ¬ï¼‰
python prepare_data.py --train 10000 --test 1000

# 4. å¼€å§‹è®­ç»ƒ
bash train_part/train_logo_lora.sh

# 5. è¯„ä¼°æ¨¡å‹
python comprehensive_eval.py \
  --checkpoint ./output/qwen3-vl-2b-logo-lora_YYYYMMDD_HHMMSS/checkpoint-XXX \
  --label "exp1_10k_r64" \
  --lora_rank 64 \
  --train_samples 10000 \
  --prompt v1 \
  --num_samples 30
```

---

## ğŸ“– è¯¦ç»†æ­¥éª¤

### æ­¥éª¤ 1: ä¸‹è½½æ•°æ®é›†

ä» [LogoDet-3K](https://github.com/Wangjing1551/LogoDet-3K-Dataset) ä¸‹è½½æ•°æ®é›†ï¼š

```bash
python download.py
```

**è¾“å‡º:**
- `logo_data/` - æ•°æ®é›†æ–‡ä»¶å¤¹
- åŒ…å« 126,923 è®­ç»ƒå›¾åƒ + 31,731 æµ‹è¯•å›¾åƒ

---

### æ­¥éª¤ 2: è½¬æ¢æ•°æ®æ ¼å¼

å°†æ•°æ®é›†è½¬æ¢ä¸º Qwen3-VL è®­ç»ƒæ ¼å¼ï¼ˆ**åªéœ€è¿è¡Œä¸€æ¬¡**ï¼‰ï¼š

```bash
python logo_data_oral/convert_logo_data.py
```

**ç”Ÿæˆæ–‡ä»¶:**
- `logo_images/` - æ‰€æœ‰å›¾åƒæ–‡ä»¶
- `logo_train.json` - å®Œæ•´è®­ç»ƒæ•°æ®ï¼ˆ380,769 æ¡æ ·æœ¬ï¼Œæ¯å¼ å›¾ 3 ä¸ªä»»åŠ¡ï¼‰
- `logo_test.json` - å®Œæ•´æµ‹è¯•æ•°æ®ï¼ˆ95,193 æ¡æ ·æœ¬ï¼‰

**ä¸‰ä¸ªè®­ç»ƒä»»åŠ¡:**
1. **åˆ†ç±»**: è¯†åˆ« logo çš„è¡Œä¸šå’Œå…¬å¸åç§°
2. **æ£€æµ‹**: å®šä½ logo çš„ bbox åæ ‡
3. **è¡Œä¸šè¯†åˆ«**: ä»…è¯†åˆ«è¡Œä¸šç±»åˆ«

---

### æ­¥éª¤ 3: å‡†å¤‡è®­ç»ƒå­é›†

ä»å®Œæ•´æ•°æ®é›†ä¸­é€‰æ‹©æŒ‡å®šæ•°é‡çš„å›¾åƒè¿›è¡Œè®­ç»ƒï¼š

```bash
# å°è§„æ¨¡æµ‹è¯•ï¼ˆ1K å›¾åƒ = 1K æ ·æœ¬ï¼‰
python prepare_data.py --train 1000 --test 200

# ä¸­ç­‰è§„æ¨¡ï¼ˆ10K å›¾åƒ = 30K æ ·æœ¬ï¼‰
python prepare_data.py --train 10000 --test 1000

# å¤§è§„æ¨¡è®­ç»ƒï¼ˆ50K å›¾åƒ = 150K æ ·æœ¬ï¼‰
python prepare_data.py --train 50000 --test 5000

# å…¨é‡è®­ç»ƒï¼ˆ126K å›¾åƒ = 380K æ ·æœ¬ï¼‰
python prepare_data.py --train 126923 --test 10000

# è‡ªå®šä¹‰éšæœºç§å­
python prepare_data.py --train 10000 --seed 123
```

**å‚æ•°è¯´æ˜:**
- `--train`: è®­ç»ƒå›¾åƒæ•°é‡ï¼ˆé»˜è®¤ 10000ï¼‰
- `--test`: æµ‹è¯•å›¾åƒæ•°é‡ï¼ˆé»˜è®¤ 1000ï¼‰
- `--seed`: éšæœºç§å­ï¼ˆé»˜è®¤ 42ï¼‰

**ç”Ÿæˆæ–‡ä»¶:**
- `train_subset.json` - è®­ç»ƒå­é›†
- `test_subset.json` - æµ‹è¯•å­é›†

**æ³¨æ„:** 
- æ— éœ€é‡å¤è½¬æ¢æ•°æ®é›†ï¼Œåªéœ€è¿è¡Œæ­¤è„šæœ¬é€‰æ‹©ä¸åŒæ•°é‡å³å¯
- ç”Ÿæˆçš„æ–‡ä»¶ä¼šä¿å­˜åœ¨**é¡¹ç›®æ ¹ç›®å½•**ä¸‹ï¼ˆä¸ `logo_train.json` åŒçº§ï¼‰

---

### æ­¥éª¤ 4: è®­ç»ƒæ¨¡å‹

å¯åŠ¨ LoRA å¾®è°ƒè®­ç»ƒï¼š

```bash
bash train_part/train_logo_lora.sh
```

**è®­ç»ƒé…ç½®ï¼ˆå¯åœ¨è„šæœ¬ä¸­ä¿®æ”¹ï¼‰:**

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `LORA_R` | 64 | LoRA rankï¼Œè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†æ˜¾å­˜å ç”¨è¶Šå¤š |
| `LORA_ALPHA` | 128 | LoRA alphaï¼Œé€šå¸¸ä¸º rank çš„ 2 å€ |
| `LORA_DROPOUT` | 0.05 | Dropout æ¯”ä¾‹ |
| `LR` | 1e-5 | å­¦ä¹ ç‡ |
| `BATCH_SIZE` | 8 | æ¯ä¸ª GPU çš„ batch size |
| `GRAD_ACCUM` | 4 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `EPOCHS` | 3 | è®­ç»ƒè½®æ•° |

**è¾“å‡ºç›®å½•:**
- `output/qwen3-vl-2b-logo-lora_YYYYMMDD_HHMMSS/` - å¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
- æ¯æ¬¡è®­ç»ƒè‡ªåŠ¨ä¿å­˜åˆ°ç‹¬ç«‹ç›®å½•ï¼Œä¸ä¼šè¦†ç›–ä¹‹å‰çš„ checkpoint

**ç›‘æ§è®­ç»ƒ:**
```bash
# æŸ¥çœ‹ TensorBoard
tensorboard --logdir output/
```

---

## ğŸ“Š è¯„ä¼°ä¸åˆ†æ

### å•æ¬¡è¯„ä¼°

è¯„ä¼°æŒ‡å®š checkpoint çš„æ€§èƒ½ï¼š

```bash
python comprehensive_eval.py \
  --checkpoint ./output/qwen3-vl-2b-logo-lora_20251119_143052/checkpoint-500 \
  --label "exp1_10k_r64" \
  --lora_rank 64 \
  --train_samples 10000 \
  --prompt v1 \
  --num_samples 30 \
  --output_dir evaluation_results
```

**å‚æ•°è¯´æ˜:**
- `--checkpoint`: LoRA checkpoint è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `--label`: å®éªŒæ ‡ç­¾ï¼Œç”¨äºæ ‡è¯†ä¸åŒå®éªŒï¼ˆå¿…éœ€ï¼‰
- `--lora_rank`: LoRA rank å¤§å°ï¼ˆé»˜è®¤ 64ï¼‰
- `--train_samples`: è®­ç»ƒæ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ 0ï¼‰
- `--prompt`: æç¤ºè¯ç‰ˆæœ¬ `v1`/`v2`/`v3`ï¼ˆé»˜è®¤ v1ï¼‰
- `--num_samples`: è¯„ä¼°æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ 30ï¼‰
- `--test_json`: æµ‹è¯•æ•°æ®é›†è·¯å¾„ï¼ˆé»˜è®¤ test_subset.jsonï¼‰
- `--output_dir`: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ evaluation_resultsï¼‰

**ç”ŸæˆæŠ¥å‘Š:**
- `evaluation_table.csv` - CSV æ ¼å¼å¯¹æ¯”è¡¨æ ¼ï¼ˆå¯ç›´æ¥ç”¨äºè®ºæ–‡ï¼‰
- `evaluation_report.md` - Markdown æ ¼å¼å®Œæ•´æŠ¥å‘Š
- `results.json` - è¯¦ç»†ç»“æœï¼ˆåŒ…å«æ‰€æœ‰å…ƒæ•°æ®ï¼‰

---

### æç¤ºè¯å¯¹æ¯”å®éªŒ

æµ‹è¯•ä¸åŒæç¤ºè¯ç‰ˆæœ¬å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼š

```bash
python test_prompts.py \
  --checkpoint ./output/qwen3-vl-2b-logo-lora_20251119_143052/checkpoint-500 \
  --label "exp1_10k_r64" \
  --lora_rank 64 \
  --train_samples 10000
```

**æç¤ºè¯ç‰ˆæœ¬:**

| ç‰ˆæœ¬ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| **v1** | åŸå§‹æç¤ºè¯ | "Identify the logo in this image..." |
| **v2** | è¯¦ç»†æç¤ºè¯ | "Analyze this image carefully. Identify the logo..." |
| **v3** | ç®€æ´æç¤ºè¯ | "What is this logo? Industry and company?" |

**ç”ŸæˆæŠ¥å‘Š:**
- `prompt_comparison_YYYYMMDD_HHMMSS/comparison_table.csv` - ä¸‰ç‰ˆæœ¬å¯¹æ¯”è¡¨æ ¼
- `prompt_comparison_YYYYMMDD_HHMMSS/comparison_report.md` - å®Œæ•´åˆ†ææŠ¥å‘Š
- æ¯ä¸ªç‰ˆæœ¬çš„ç‹¬ç«‹è¯„ä¼°ç»“æœ

**æŠ¥å‘ŠåŒ…å«:**
- ä¸‰ç§æç¤ºè¯åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯¹æ¯”
- Base æ¨¡å‹ vs LoRA æ¨¡å‹çš„æå‡å¯¹æ¯”
- æœ€ä½³æç¤ºè¯ç‰ˆæœ¬æ¨è

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
classVLM/
â”œâ”€â”€ download.py                      # æ•°æ®é›†ä¸‹è½½è„šæœ¬
â”œâ”€â”€ prepare_data.py                  # è®­ç»ƒæ•°æ®å‡†å¤‡ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼‰
â”œâ”€â”€ comprehensive_eval.py            # ç»¼åˆè¯„ä¼°è„šæœ¬ï¼ˆæ”¯æŒæ ‡ç­¾å’Œæç¤ºè¯ï¼‰
â”œâ”€â”€ test_prompts.py                  # æç¤ºè¯å¯¹æ¯”å®éªŒ
â”œâ”€â”€ logo_data_oral/
â”‚   â””â”€â”€ convert_logo_data.py        # æ•°æ®æ ¼å¼è½¬æ¢ï¼ˆä¸€æ¬¡æ€§ï¼‰
â”œâ”€â”€ train_part/
â”‚   â””â”€â”€ train_logo_lora.sh          # è®­ç»ƒè„šæœ¬ï¼ˆè‡ªåŠ¨æ·»åŠ æ—¶é—´æˆ³ï¼‰
â”œâ”€â”€ logo_data/                       # åŸå§‹æ•°æ®é›†
â”œâ”€â”€ logo_images/                     # è½¬æ¢åçš„å›¾åƒ
â”œâ”€â”€ logo_train.json                  # å®Œæ•´è®­ç»ƒæ•°æ®ï¼ˆ380K æ ·æœ¬ï¼‰
â”œâ”€â”€ logo_test.json                   # å®Œæ•´æµ‹è¯•æ•°æ®ï¼ˆ95K æ ·æœ¬ï¼‰
â”œâ”€â”€ train_subset.json                # è®­ç»ƒå­é›†
â”œâ”€â”€ test_subset.json                 # æµ‹è¯•å­é›†
â””â”€â”€ output/                          # è®­ç»ƒè¾“å‡ºï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
```

---

## ğŸ’¡ ä½¿ç”¨å»ºè®®

1. **é¦–æ¬¡ä½¿ç”¨**: 
   - å¿…é¡»å…ˆé…ç½®ç»å¯¹è·¯å¾„ï¼ˆè§[å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)ï¼‰
   - å»ºè®®ä» 1K æ ·æœ¬å¼€å§‹å¿«é€ŸéªŒè¯æµç¨‹
2. **æ­£å¼è®­ç»ƒ**: æ¨èä½¿ç”¨ 10K-50K æ ·æœ¬ï¼Œå¹³è¡¡æ•ˆæœå’Œæ—¶é—´
3. **å…¨é‡è®­ç»ƒ**: 126K æ ·æœ¬éœ€è¦è¾ƒé•¿è®­ç»ƒæ—¶é—´ï¼Œé€‚åˆæœ€ç»ˆæ¨¡å‹
4. **æç¤ºè¯ä¼˜åŒ–**: ä½¿ç”¨ `test_prompts.py` æ‰¾åˆ°æœ€ä½³æç¤ºè¯ç‰ˆæœ¬
5. **å®éªŒç®¡ç†**: ä½¿ç”¨æœ‰æ„ä¹‰çš„ `--label` æ ‡è¯†ä¸åŒå®éªŒé…ç½®
6. **å¤šæœºè®­ç»ƒ**: å¦‚æœåœ¨å¤šå°æœºå™¨ä¸Šè®­ç»ƒï¼Œæ¯å°æœºå™¨éƒ½éœ€è¦ä¿®æ”¹è·¯å¾„é…ç½®

---

## ğŸ¯ è¯„ä¼°æŒ‡æ ‡

| ä»»åŠ¡ | æŒ‡æ ‡ | è¯´æ˜ |
|------|------|------|
| åˆ†ç±» | å‡†ç¡®ç‡ | Logo è¡Œä¸šå’Œå…¬å¸åç§°è¯†åˆ«å‡†ç¡®ç‡ |
| æ£€æµ‹ | IoU | Bbox å®šä½çš„å¹³å‡ IoU |
| è¡Œä¸šè¯†åˆ« | å‡†ç¡®ç‡ | ä»…è¡Œä¸šç±»åˆ«çš„è¯†åˆ«å‡†ç¡®ç‡ |

---

## ğŸ“ å¼•ç”¨

å¦‚æœä½¿ç”¨ LogoDet-3K æ•°æ®é›†ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{wang2020logodet3k,
  title={LogoDet-3K: A Large-Scale Image Dataset for Logo Detection},
  author={Wang, Jing and others},
  year={2020}
}
```