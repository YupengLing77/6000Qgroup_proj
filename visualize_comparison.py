"""
å¯è§†åŒ–å¯¹æ¯”è®­ç»ƒå‰åçš„æ£€æµ‹æ•ˆæœ
åœ¨å›¾åƒä¸Šæ ‡æ³¨ bbox å¹¶ä¿å­˜å¯¹æ¯”å›¾
"""
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from peft import PeftModel
from PIL import Image, ImageDraw, ImageFont
import json
import re
import sys
import os

def parse_bbox(response_text):
    """ä»æ¨¡å‹å›ç­”ä¸­æå– bbox åæ ‡"""
    # å°è¯•åŒ¹é… JSON æ ¼å¼: {"bbox_2d": [x1, y1, x2, y2]}
    json_match = re.search(r'\{\s*"bbox_2d"\s*:\s*\[(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\]', response_text)
    if json_match:
        return [int(x) for x in json_match.groups()]
    
    # å°è¯•åŒ¹é…æ•°ç»„æ ¼å¼: [x1, y1, x2, y2]
    array_match = re.search(r'\[(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\]', response_text)
    if array_match:
        return [int(x) for x in array_match.groups()]
    
    return None

def load_model_and_predict(image_path, model_path, lora_checkpoint=None, task_type="detect"):
    """åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹"""
    # åŠ è½½æ¨¡å‹
    model = AutoModelForVision2Seq.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    if lora_checkpoint:
        model = PeftModel.from_pretrained(model, lora_checkpoint)
    
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
    model.eval()
    
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert("RGB")
    
    # æ„å»ºä¸åŒä»»åŠ¡çš„æç¤ºè¯
    if task_type == "detect":
        prompt = "Locate the logo in this image and output the bbox coordinates in JSON format."
    elif task_type == "classify":
        prompt = "Identify the logo in this image. What is the industry and company name?"
    else:
        prompt = "What industry does this logo belong to?"
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt}
            ]
        }
    ]
    
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], images=[image], return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        output_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)
    
    generated_text = processor.batch_decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    answer = generated_text.split("assistant\n")[-1].strip()
    
    return answer, image

def draw_bbox_on_image(image, bbox, color, label, thickness=3):
    """åœ¨å›¾åƒä¸Šç»˜åˆ¶ bbox"""
    draw = ImageDraw.Draw(image)
    
    if bbox:
        x1, y1, x2, y2 = bbox
        # ç»˜åˆ¶çŸ©å½¢æ¡†
        draw.rectangle([x1, y1, x2, y2], outline=color, width=thickness)
        
        # æ·»åŠ æ ‡ç­¾
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 20)
        except:
            font = ImageFont.load_default()
        
        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
        text_bbox = draw.textbbox((x1, y1-25), label, font=font)
        draw.rectangle([text_bbox[0]-2, text_bbox[1]-2, text_bbox[2]+2, text_bbox[3]+2], fill=color)
        draw.text((x1, y1-25), label, fill="white", font=font)
        
        # æ˜¾ç¤ºåæ ‡
        coord_text = f"[{x1},{y1},{x2},{y2}]"
        draw.text((x1, y2+5), coord_text, fill=color, font=font)
    
    return image

def create_comparison_visualization(image_path, base_model_path, lora_checkpoint, output_dir="visualization"):
    """åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–"""
    os.makedirs(output_dir, exist_ok=True)
    
    image_name = os.path.basename(image_path)
    print(f"\n{'='*80}")
    print(f"å¯è§†åŒ–å¯¹æ¯”: {image_name}")
    print(f"{'='*80}\n")
    
    # 1. åŸºç¡€æ¨¡å‹é¢„æµ‹
    print("ğŸ”µ æµ‹è¯•åŸºç¡€æ¨¡å‹ï¼ˆæœªè®­ç»ƒï¼‰...")
    base_response, original_image = load_model_and_predict(
        image_path, base_model_path, None, "detect"
    )
    base_bbox = parse_bbox(base_response)
    print(f"åŸºç¡€æ¨¡å‹å›ç­”: {base_response}")
    print(f"æå–çš„ bbox: {base_bbox}")
    
    # 2. LoRA æ¨¡å‹é¢„æµ‹
    print("\nğŸŸ¢ æµ‹è¯• LoRA æ¨¡å‹...")
    lora_response, _ = load_model_and_predict(
        image_path, base_model_path, lora_checkpoint, "detect"
    )
    lora_bbox = parse_bbox(lora_response)
    print(f"LoRA æ¨¡å‹å›ç­”: {lora_response}")
    print(f"æå–çš„ bbox: {lora_bbox}")
    
    # 3. è·å–åˆ†ç±»ç»“æœ
    print("\nğŸ“Š è·å–åˆ†ç±»ç»“æœ...")
    base_classify, _ = load_model_and_predict(image_path, base_model_path, None, "classify")
    lora_classify, _ = load_model_and_predict(image_path, base_model_path, lora_checkpoint, "classify")
    
    # 4. åˆ›å»ºå¯è§†åŒ–å¯¹æ¯”å›¾
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾åƒ...")
    
    # åŸºç¡€æ¨¡å‹å¯è§†åŒ–
    base_image = original_image.copy()
    base_image = draw_bbox_on_image(
        base_image, base_bbox, 
        color="blue", 
        label="Base Model",
        thickness=4
    )
    
    # LoRA æ¨¡å‹å¯è§†åŒ–
    lora_image = original_image.copy()
    lora_image = draw_bbox_on_image(
        lora_image, lora_bbox, 
        color="green", 
        label="LoRA Model",
        thickness=4
    )
    
    # å åŠ å¯¹æ¯”ï¼ˆä¸¤ä¸ªæ¡†éƒ½æ˜¾ç¤ºï¼‰
    combined_image = original_image.copy()
    if base_bbox:
        combined_image = draw_bbox_on_image(
            combined_image, base_bbox,
            color="blue",
            label="Base",
            thickness=3
        )
    if lora_bbox:
        combined_image = draw_bbox_on_image(
            combined_image, lora_bbox,
            color="green",
            label="LoRA",
            thickness=3
        )
    
    # åˆ›å»ºæ‹¼æ¥å›¾ï¼ˆæ¨ªå‘ï¼‰
    width, height = original_image.size
    comparison = Image.new('RGB', (width * 3, height), (255, 255, 255))
    comparison.paste(base_image, (0, 0))
    comparison.paste(lora_image, (width, 0))
    comparison.paste(combined_image, (width * 2, 0))
    
    # æ·»åŠ æ ‡é¢˜
    draw = ImageDraw.Draw(comparison)
    try:
        title_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 30)
    except:
        title_font = ImageFont.load_default()
    
    # ä¿å­˜ç»“æœ
    output_path = os.path.join(output_dir, f"comparison_{image_name}")
    comparison.save(output_path)
    
    # ä¿å­˜å•ç‹¬çš„å›¾åƒ
    base_image.save(os.path.join(output_dir, f"base_{image_name}"))
    lora_image.save(os.path.join(output_dir, f"lora_{image_name}"))
    combined_image.save(os.path.join(output_dir, f"overlay_{image_name}"))
    
    print(f"\nâœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜:")
    print(f"   - å¯¹æ¯”å›¾: {output_path}")
    print(f"   - åŸºç¡€æ¨¡å‹: {output_dir}/base_{image_name}")
    print(f"   - LoRAæ¨¡å‹: {output_dir}/lora_{image_name}")
    print(f"   - å åŠ å›¾: {output_dir}/overlay_{image_name}")
    
    # æ‰“å°åˆ†ç±»å¯¹æ¯”
    print(f"\n{'='*80}")
    print("åˆ†ç±»ç»“æœå¯¹æ¯”:")
    print(f"{'='*80}")
    print(f"ğŸ”µ åŸºç¡€æ¨¡å‹: {base_classify[:100]}...")
    print(f"ğŸŸ¢ LoRAæ¨¡å‹: {lora_classify[:100]}...")
    
    return {
        'base_bbox': base_bbox,
        'lora_bbox': lora_bbox,
        'base_classify': base_classify,
        'lora_classify': lora_classify,
        'output_path': output_path
    }

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python visualize_comparison.py <å›¾åƒè·¯å¾„>")
        print("ç¤ºä¾‹: python visualize_comparison.py logo_images/test/test_000000.jpg")
        sys.exit(1)
    
    image_path = sys.argv[1]
    BASE_MODEL = "Qwen/Qwen3-VL-2B-Instruct"
    LORA_CHECKPOINT = "/home/jiahuawang/test/classVLM/output/qwen3-vl-2b-logo-lora/checkpoint-564"
    
    result = create_comparison_visualization(image_path, BASE_MODEL, LORA_CHECKPOINT)
    
    print(f"\n{'='*80}")
    print("âœ… å®Œæˆï¼è¯·æŸ¥çœ‹ visualization/ ç›®å½•")
    print(f"{'='*80}")
