"""
å¿«é€Ÿæµ‹è¯•å•å¼ å›¾åƒ - å¯¹æ¯”è®­ç»ƒå‰åæ•ˆæœ
"""
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from peft import PeftModel
from PIL import Image
import sys

def test_single_image(image_path, use_lora=True):
    """æµ‹è¯•å•å¼ å›¾åƒ"""
    
    BASE_MODEL = "Qwen/Qwen3-VL-2B-Instruct"
    LORA_CHECKPOINT = "/home/jiahuawang/test/classVLM/output/qwen3-vl-2b-logo-lora/checkpoint-564"
    
    print(f"\n{'='*80}")
    if use_lora:
        print("ä½¿ç”¨ LoRA å¾®è°ƒåçš„æ¨¡å‹")
        print(f"Checkpoint: {LORA_CHECKPOINT}")
    else:
        print("ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆæœªè®­ç»ƒï¼‰")
    print(f"{'='*80}\n")
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½æ¨¡å‹ä¸­...")
    base_model = AutoModelForVision2Seq.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    if use_lora:
        model = PeftModel.from_pretrained(base_model, LORA_CHECKPOINT)
    else:
        model = base_model
    
    processor = AutoProcessor.from_pretrained(BASE_MODEL)
    model.eval()
    
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert("RGB")
    print(f"å›¾åƒ: {image_path}")
    print(f"å°ºå¯¸: {image.size}")
    
    # å®šä¹‰æµ‹è¯•ä»»åŠ¡
    tasks = [
        ("åˆ†ç±»ä»»åŠ¡", "Identify the logo in this image. What is the industry and company name?"),
        ("è¡Œä¸šè¯†åˆ«", "What industry does this logo belong to?"),
        ("ç›®æ ‡æ£€æµ‹", "Locate the logo in this image and output the bbox coordinates in JSON format.")
    ]
    
    print(f"\n{'='*80}")
    print("å¼€å§‹æµ‹è¯•")
    print(f"{'='*80}\n")
    
    for task_name, prompt in tasks:
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        inputs = processor(
            text=[text], 
            images=[image], 
            return_tensors="pt"
        ).to(model.device)
        
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=False
            )
        
        generated_text = processor.batch_decode(
            output_ids, 
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        answer = generated_text.split("assistant\n")[-1].strip()
        
        print(f"ğŸ“Œ {task_name}")
        print(f"é—®é¢˜: {prompt}")
        print(f"å›ç­”: {answer}")
        print()

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python quick_test.py <å›¾åƒè·¯å¾„>")
        print("ç¤ºä¾‹: python quick_test.py logo_images/test/test_000001.jpg")
        sys.exit(1)
    
    image_path = sys.argv[1]
    
    # å…ˆæµ‹è¯•åŸºç¡€æ¨¡å‹
    print("\n" + "ğŸ”µ "*40)
    print("æµ‹è¯• 1: åŸºç¡€æ¨¡å‹ï¼ˆæœªè®­ç»ƒï¼‰")
    print("ğŸ”µ "*40)
    test_single_image(image_path, use_lora=False)
    
    # å†æµ‹è¯• LoRA æ¨¡å‹
    print("\n" + "ğŸŸ¢ "*40)
    print("æµ‹è¯• 2: LoRA å¾®è°ƒæ¨¡å‹")
    print("ğŸŸ¢ "*40)
    test_single_image(image_path, use_lora=True)
    
    print("\n" + "="*80)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("="*80)
