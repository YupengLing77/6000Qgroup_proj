"""
å¯¹æ¯”è®­ç»ƒå‰åæ¨¡å‹çš„æ•ˆæœ
æ¯”è¾ƒåŸºç¡€æ¨¡å‹ vs LoRA å¾®è°ƒåæ¨¡å‹
"""
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from peft import PeftModel
from PIL import Image
import json
import sys
from tqdm import tqdm

def load_base_model(model_path):
    """åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆæœªè®­ç»ƒï¼‰"""
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {model_path}")
    model = AutoModelForVision2Seq.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
    return model, processor

def load_lora_model(base_model_path, lora_checkpoint):
    """åŠ è½½ LoRA å¾®è°ƒåçš„æ¨¡å‹"""
    print(f"åŠ è½½ LoRA æ¨¡å‹: {lora_checkpoint}")
    base_model = AutoModelForVision2Seq.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    model = PeftModel.from_pretrained(base_model, lora_checkpoint)
    processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)
    return model, processor

def predict(model, processor, image_path, prompt):
    """ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    image = Image.open(image_path).convert("RGB")
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt}
            ]
        }
    ]
    
    text = processor.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    inputs = processor(
        text=[text], 
        images=[image], 
        return_tensors="pt"
    ).to(model.device)
    
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False
        )
    
    generated_text = processor.batch_decode(
        output_ids, 
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]
    
    answer = generated_text.split("assistant\n")[-1].strip()
    return answer

def compare_on_test_set(base_model, base_processor, lora_model, lora_processor, 
                        test_json, num_samples=10):
    """åœ¨æµ‹è¯•é›†ä¸Šå¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹"""
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open(test_json, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    # åªæµ‹è¯•å‰ num_samples ä¸ª
    test_samples = test_data[:num_samples]
    
    print(f"\n{'='*80}")
    print(f"åœ¨ {num_samples} ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šå¯¹æ¯”æ¨¡å‹æ•ˆæœ")
    print(f"{'='*80}\n")
    
    results = []
    
    for i, sample in enumerate(tqdm(test_samples, desc="æµ‹è¯•ä¸­")):
        image_path = sample['image']
        prompt = sample['conversations'][0]['value'].replace('<image>\n', '')
        ground_truth = sample['conversations'][1]['value']
        
        # åŸºç¡€æ¨¡å‹é¢„æµ‹
        base_answer = predict(base_model, base_processor, image_path, prompt)
        
        # LoRA æ¨¡å‹é¢„æµ‹
        lora_answer = predict(lora_model, lora_processor, image_path, prompt)
        
        result = {
            'id': i + 1,
            'image': image_path,
            'prompt': prompt,
            'ground_truth': ground_truth,
            'base_model': base_answer,
            'lora_model': lora_answer
        }
        results.append(result)
        
        # æ‰“å°å¯¹æ¯”
        print(f"\n{'â”€'*80}")
        print(f"æ ·æœ¬ {i+1}/{num_samples}")
        print(f"{'â”€'*80}")
        print(f"ğŸ“· å›¾åƒ: {image_path.split('/')[-1]}")
        print(f"â“ é—®é¢˜: {prompt[:100]}...")
        print(f"\nâœ… æ ‡å‡†ç­”æ¡ˆ:")
        print(f"   {ground_truth}")
        print(f"\nğŸ”µ åŸºç¡€æ¨¡å‹:")
        print(f"   {base_answer}")
        print(f"\nğŸŸ¢ LoRAæ¨¡å‹:")
        print(f"   {lora_answer}")
        
        # ç®€å•è¯„ä¼°ï¼ˆæ˜¯å¦åŒ…å«å…³é”®è¯ï¼‰
        gt_lower = ground_truth.lower()
        base_match = any(word in base_answer.lower() for word in gt_lower.split()[:3])
        lora_match = any(word in lora_answer.lower() for word in gt_lower.split()[:3])
        
        if lora_match and not base_match:
            print(f"   âœ¨ LoRA æ¨¡å‹æ›´å‡†ç¡®ï¼")
        elif base_match and not lora_match:
            print(f"   âš ï¸  åŸºç¡€æ¨¡å‹æ›´å‡†ç¡®")
        elif lora_match and base_match:
            print(f"   âœ“ ä¸¤ä¸ªæ¨¡å‹éƒ½æ­£ç¡®")
    
    return results

def save_comparison_report(results, output_file="comparison_report.json"):
    """ä¿å­˜å¯¹æ¯”æŠ¥å‘Š"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nå¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {output_file}")

if __name__ == "__main__":
    # é…ç½®
    BASE_MODEL_PATH = "Qwen/Qwen3-VL-2B-Instruct"
    LORA_CHECKPOINT = "/home/jiahuawang/test/classVLM/output/qwen3-vl-2b-logo-lora/checkpoint-564"
    TEST_JSON = "logo_test.json"
    NUM_SAMPLES = 10  # æµ‹è¯•æ ·æœ¬æ•°é‡
    
    print("="*80)
    print("æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    print(f"åŸºç¡€æ¨¡å‹: {BASE_MODEL_PATH}")
    print(f"LoRA æ¨¡å‹: {LORA_CHECKPOINT}")
    print(f"æµ‹è¯•æ•°æ®: {TEST_JSON}")
    print(f"æµ‹è¯•æ ·æœ¬: {NUM_SAMPLES}")
    print("="*80)
    
    # åŠ è½½æ¨¡å‹
    print("\n[1/3] åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model, base_processor = load_base_model(BASE_MODEL_PATH)
    base_model.eval()
    
    print("\n[2/3] åŠ è½½ LoRA æ¨¡å‹...")
    lora_model, lora_processor = load_lora_model(BASE_MODEL_PATH, LORA_CHECKPOINT)
    lora_model.eval()
    
    # å¼€å§‹å¯¹æ¯”
    print("\n[3/3] å¼€å§‹å¯¹æ¯”æµ‹è¯•...")
    results = compare_on_test_set(
        base_model, base_processor,
        lora_model, lora_processor,
        TEST_JSON, NUM_SAMPLES
    )
    
    # ä¿å­˜ç»“æœ
    save_comparison_report(results)
    
    print("\n" + "="*80)
    print("âœ… å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
    print("="*80)
